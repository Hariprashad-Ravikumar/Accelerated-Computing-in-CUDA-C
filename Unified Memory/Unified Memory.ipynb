{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a report file which can be used in a variety of manners, including for use in visual profiling with Nsight Systems, which we will look at in more detail in the following section.\n",
    "\n",
    "Here we use the `--stats=true` flag to indicate we would like summary statistics printed. In this section this summary will be the focus of our attention. There is quite a lot of information printed:\n",
    "\n",
    "- Operating System Runtime Summary (`osrt_sum`)\n",
    "- **CUDA API Summary (`cuda_api_sum`)**\n",
    "- **CUDA Kernel Summary (`cuda_gpu_kern_sum`)**\n",
    "- **CUDA Memory Time Operation Summary (`cuda_gpu_mem_time_sum`)**\n",
    "- **CUDA Memory Size Operation Summary (`cuda_gpu_mem_size_sum`)**\n",
    "\n",
    "In this section you will primarily be using the 4 summaries in **bold** above. In the next section, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `cuda_gpu_kern_sum` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-a20e.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.3       6054131773        317  19098207.5  10070523.0      2190  100163795   27334856.3  poll                  \n",
      "      8.9        599393037        283   2117996.6   2064625.0       200   20453018    1592301.4  sem_timedwait         \n",
      "      0.5         31165059        499     62455.0     10850.0       380    8444285     397256.9  ioctl                 \n",
      "      0.3         19559006         24    814958.6      4700.0       920    7488640    2202600.9  mmap                  \n",
      "      0.0           912051         27     33779.7      3971.0      2690     596951     113213.9  mmap64                \n",
      "      0.0           502768         44     11426.5     10175.0      3370      34341       6496.0  open64                \n",
      "      0.0           169436          4     42359.0     41061.5     32051      55262      11290.3  pthread_create        \n",
      "      0.0           138515         11     12592.3     12931.0       860      19941       4906.8  write                 \n",
      "      0.0           131524         29      4535.3      2820.0       890      22791       4745.9  fopen                 \n",
      "      0.0            51521         11      4683.7      2960.0      1300      19551       5062.9  munmap                \n",
      "      0.0            51102         26      1965.5        70.0        60      49352       9665.0  fgets                 \n",
      "      0.0            33292         52       640.2       465.0       160       6331        863.5  fcntl                 \n",
      "      0.0            32891          6      5481.8      4705.0      2490       8611       2406.4  open                  \n",
      "      0.0            23350         22      1061.4       910.0       510       3710        676.0  fclose                \n",
      "      0.0            21921         14      1565.8      1315.0       850       3400        727.9  read                  \n",
      "      0.0            16320          2      8160.0      8160.0      5470      10850       3804.2  socket                \n",
      "      0.0            11371          1     11371.0     11371.0     11371      11371          0.0  connect               \n",
      "      0.0             8830          5      1766.0      1050.0        70       5430       2209.0  fread                 \n",
      "      0.0             7330          1      7330.0      7330.0      7330       7330          0.0  pipe2                 \n",
      "      0.0             6060         64        94.7        85.0        40        220         52.0  pthread_mutex_trylock \n",
      "      0.0             2500          1      2500.0      2500.0      2500       2500          0.0  bind                  \n",
      "      0.0             1240          1      1240.0      1240.0      1240       1240          0.0  listen                \n",
      "      0.0              300          1       300.0       300.0       300        300          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     95.1       2467977457          1  2467977457.0  2467977457.0  2467977457  2467977457          0.0  cudaDeviceSynchronize\n",
      "      4.2        108740475          3    36246825.0       28231.0       14211   108698033   62744587.1  cudaMallocManaged    \n",
      "      0.8         19613478          3     6537826.0     6170102.0     5922894     7520482     859934.6  cudaFree             \n",
      "      0.0            55482          1       55482.0       55482.0       55482       55482          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2467970041          1  2467970041.0  2467970041.0  2467970041  2467970041          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34146842   2304   14820.7    4384.0      1983     80481      22496.1  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11058513    768   14399.1    3728.0      1279     80801      22783.9  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-5817.qdstrm'\n",
      "[1/8] [========================100%] report3.nsys-rep\n",
      "[2/8] [========================100%] report3.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report3.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6054257553        317  19098604.3  10068857.0      2730  100151224   27331801.8  poll                  \n",
      "      8.8        591188074        283   2089003.8   2064748.0       190   20440756    1303766.2  sem_timedwait         \n",
      "      0.5         31730396        499     63588.0     11350.0       370    8162366     387434.5  ioctl                 \n",
      "      0.3         19059177         24    794132.4      6565.0       920    7143388    2138517.4  mmap                  \n",
      "      0.0           974986         27     36110.6      3910.0      3060     636464     120661.8  mmap64                \n",
      "      0.0           485309         44     11029.8     10695.0      3571      26951       4059.9  open64                \n",
      "      0.0           184827          4     46206.8     45017.0     36281      58512      10496.4  pthread_create        \n",
      "      0.0           163236         29      5628.8      3611.0      1470      32121       6639.0  fopen                 \n",
      "      0.0           134944         11     12267.6     13680.0      1070      17700       4203.0  write                 \n",
      "      0.0           102434         11      9312.2      3710.0      1240      38161      11368.8  munmap                \n",
      "      0.0            51172         26      1968.2        70.0        60      49362       9666.5  fgets                 \n",
      "      0.0            37321          6      6220.2      6715.5      2850       8960       2273.5  open                  \n",
      "      0.0            34482         52       663.1       460.0       170       6240        846.2  fcntl                 \n",
      "      0.0            25194         22      1145.2       990.0       520       3271        597.8  fclose                \n",
      "      0.0            21272         14      1519.4      1230.5       860       3431        821.0  read                  \n",
      "      0.0            19200          2      9600.0      9600.0      5580      13620       5685.1  socket                \n",
      "      0.0            11900          1     11900.0     11900.0     11900      11900          0.0  connect               \n",
      "      0.0             8970          5      1794.0      1700.0        70       3420       1532.7  fread                 \n",
      "      0.0             7630          1      7630.0      7630.0      7630       7630          0.0  pipe2                 \n",
      "      0.0             6140         64        95.9        85.0        40        340         59.6  pthread_mutex_trylock \n",
      "      0.0             2540          1      2540.0      2540.0      2540       2540          0.0  bind                  \n",
      "      0.0             1730          1      1730.0      1730.0      1730       1730          0.0  listen                \n",
      "      0.0              310          1       310.0       310.0       310        310          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     95.0       2471882832          1  2471882832.0  2471882832.0  2471882832  2471882832          0.0  cudaDeviceSynchronize\n",
      "      4.2        110161688          3    36720562.7       55522.0       14610   110091556   63541147.4  cudaMallocManaged    \n",
      "      0.7         19089994          3     6363331.3     6082637.0     5820008     7187349     725601.4  cudaFree             \n",
      "      0.0            46722          1       46722.0       46722.0       46722       46722          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2471874598          1  2471874598.0  2471874598.0  2471874598  2471874598          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.6         34254803   2304   14867.5    6272.0      1855    136770      22598.8  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.4         11063975    768   14406.2    3727.5      1279     80673      22789.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report3.nsys-rep\n",
      "    /dli/task/report3.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-5f1f.qdstrm'\n",
      "[1/8] [========================100%] report4.nsys-rep\n",
      "[2/8] [========================100%] report4.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report4.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6044050256        316  19126741.3  10072791.0      2180  100149655   27372945.8  poll                  \n",
      "      8.8        587988047        282   2085064.0   2064385.0       180   20433163    1266714.3  sem_timedwait         \n",
      "      0.5         31292437        499     62710.3     10351.0       380    8418636     397102.5  ioctl                 \n",
      "      0.3         19620511         24    817521.3      4805.0       900    7262542    2201871.8  mmap                  \n",
      "      0.0           988102         27     36596.4      3860.0      2800     627475     118913.0  mmap64                \n",
      "      0.0           495505         44     11261.5      9975.0      3280      32371       6152.4  open64                \n",
      "      0.0           161906          4     40476.5     39291.5     31081      52242       9014.3  pthread_create        \n",
      "      0.0           143694         11     13063.1     13070.0       880      24401       6180.6  write                 \n",
      "      0.0           131168         29      4523.0      3240.0       891      22361       4824.2  fopen                 \n",
      "      0.0            50382         26      1937.8        70.0        60      48662       9529.9  fgets                 \n",
      "      0.0            42632         10      4263.2      3475.0      1250      11161       2761.1  munmap                \n",
      "      0.0            33480          6      5580.0      4780.0      2440       8890       2437.5  open                  \n",
      "      0.0            33112         52       636.8       475.0       150       5550        785.8  fcntl                 \n",
      "      0.0            24971         14      1783.6      1230.0       350       8070       2033.4  read                  \n",
      "      0.0            23071         22      1048.7       920.0       490       3610        668.8  fclose                \n",
      "      0.0            15801          2      7900.5      7900.5      4480      11321       4837.3  socket                \n",
      "      0.0            11491          1     11491.0     11491.0     11491      11491          0.0  connect               \n",
      "      0.0             7151          1      7151.0      7151.0      7151       7151          0.0  pipe2                 \n",
      "      0.0             7040          5      1408.0      1130.0        70       3590       1467.4  fread                 \n",
      "      0.0             6270         64        98.0        50.0        40        360         67.9  pthread_mutex_trylock \n",
      "      0.0             2520          1      2520.0      2520.0      2520       2520          0.0  bind                  \n",
      "      0.0             1340          1      1340.0      1340.0      1340       1340          0.0  listen                \n",
      "      0.0              490          1       490.0       490.0       490        490          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     95.0       2467915325          1  2467915325.0  2467915325.0  2467915325  2467915325          0.0  cudaDeviceSynchronize\n",
      "      4.2        109092141          3    36364047.0       29211.0       14070   109048860   62946895.0  cudaMallocManaged    \n",
      "      0.8         19741976          3     6580658.7     6248233.0     6167059     7326684     647350.5  cudaFree             \n",
      "      0.0           127825          1      127825.0      127825.0      127825      127825          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2467977602          1  2467977602.0  2467977602.0  2467977602  2467977602          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34157544   2304   14825.3    4415.5      1983     80226      22490.8  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11065951    768   14408.8    3727.5      1311     80769      22784.7  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report4.nsys-rep\n",
      "    /dli/task/report4.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"deviceId\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"multiProcessorCount\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"computeCapabilityMajor\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"computeCapabilityMinor\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"warpSize\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"deviceId\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"multiProcessorCount\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"computeCapabilityMajor\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"computeCapabilityMinor\"\u001b[0m is used before its value is set\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m04-device-properties/01-get-device-properties.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"warpSize\"\u001b[0m is used before its value is set\n",
      "\n",
      "Device ID: 21940\n",
      "Number of SMs: 685375104\n",
      "Compute Capability Major: 32765\n",
      "Compute Capability Minor: 0\n",
      "Warp Size: 0\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-1a9a.qdstrm'\n",
      "[1/8] [========================100%] report5.nsys-rep\n",
      "[2/8] [========================100%] report5.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report5.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.3       6054286581        317  19098695.8  10072814.0      2180  100147955   27332147.4  poll                  \n",
      "      8.9        593321275        283   2096541.6   2065453.0       140   20425739    1364023.0  sem_timedwait         \n",
      "      0.5         32033830        499     64196.1     10610.0       380    8296662     411322.6  ioctl                 \n",
      "      0.3         19411649         24    808818.7      5530.0       880    7200449    2179167.3  mmap                  \n",
      "      0.0           891775         27     33028.7      4360.0      3010     564822     107001.3  mmap64                \n",
      "      0.0           498932         44     11339.4      9765.5      3640      33391       5653.9  open64                \n",
      "      0.0           182837          4     45709.3     43146.5     32461      64083      15484.7  pthread_create        \n",
      "      0.0           149325         29      5149.1      3640.0      1440      22331       5028.1  fopen                 \n",
      "      0.0           143506         11     13046.0     13391.0       900      22481       5139.9  write                 \n",
      "      0.0            70504         11      6409.5      3800.0      1350      17091       5656.6  munmap                \n",
      "      0.0            58022         26      2231.6        70.0        60      56162      10999.7  fgets                 \n",
      "      0.0            37383          6      6230.5      7101.0      2490       8571       2513.7  open                  \n",
      "      0.0            34070         52       655.2       485.0       170       5930        805.1  fcntl                 \n",
      "      0.0            27103         22      1232.0       950.0       480       3460        744.8  fclose                \n",
      "      0.0            23161         14      1654.4      1200.0       840       4001       1007.8  read                  \n",
      "      0.0            17281          2      8640.5      8640.5      3990      13291       6576.8  socket                \n",
      "      0.0            11061          1     11061.0     11061.0     11061      11061          0.0  connect               \n",
      "      0.0             9500          5      1900.0      1730.0        70       5390       2161.5  fread                 \n",
      "      0.0             6630          1      6630.0      6630.0      6630       6630          0.0  pipe2                 \n",
      "      0.0             6380         64        99.7       120.0        40        420         62.9  pthread_mutex_trylock \n",
      "      0.0             2920          1      2920.0      2920.0      2920       2920          0.0  bind                  \n",
      "      0.0             1150          1      1150.0      1150.0      1150       1150          0.0  listen                \n",
      "      0.0              670          1       670.0       670.0       670        670          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     95.1       2469577783          1  2469577783.0  2469577783.0  2469577783  2469577783          0.0  cudaDeviceSynchronize\n",
      "      4.2        109024821          3    36341607.0       31181.0       13821   108979819   62906537.5  cudaMallocManaged    \n",
      "      0.7         19466970          3     6488990.0     6129586.0     6097024     7240360     650909.2  cudaFree             \n",
      "      0.0            46882          1       46882.0       46882.0       46882       46882          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2469568019          1  2469568019.0  2469568019.0  2469568019  2469568019          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34169119   2304   14830.3    6272.0      1823     80449      22495.5  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11066990    768   14410.1    3728.0      1183     80545      22784.8  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report5.nsys-rep\n",
      "    /dli/task/report5.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating '/tmp/nsys-report-d1fe.qdstrm'\n",
      "[1/8] [========================100%] report6.nsys-rep\n",
      "[2/8] [========================100%] report6.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report6.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ----------------------\n",
      "     66.8        130301202         15  8686746.8  1933418.0      2690  51086925   13199000.7  poll                  \n",
      "     16.2         31581071         13  2429313.2    38071.0       200  20453891    6100803.0  sem_timedwait         \n",
      "     15.7         30598037        482    63481.4    11495.5       380   8232425     395190.1  ioctl                 \n",
      "      0.5           937490         27    34721.9     4680.0      3080    584734     110732.6  mmap64                \n",
      "      0.3           508809         44    11563.8    10465.5      3600     35732       5726.8  open64                \n",
      "      0.1           186689          4    46672.3    44417.0     35412     62443      13215.7  pthread_create        \n",
      "      0.1           175919         11    15992.6    12061.0      1130     50952      12547.5  write                 \n",
      "      0.1           172951         29     5963.8     3730.0       910     36791       7875.3  fopen                 \n",
      "      0.1           146626         18     8145.9     6750.5       930     34121       8271.2  mmap                  \n",
      "      0.0            77873         26     2995.1       70.0        50     76073      14905.0  fgets                 \n",
      "      0.0            66533          7     9504.7     2950.0      1920     46832      16527.5  munmap                \n",
      "      0.0            41060          6     6843.3     8050.0      2540      9470       2731.6  open                  \n",
      "      0.0            35390         52      680.6      465.0       150      6540        910.4  fcntl                 \n",
      "      0.0            28800         22     1309.1      980.0       530      4260        854.5  fclose                \n",
      "      0.0            24771         14     1769.4     1355.0       721      4760       1109.8  read                  \n",
      "      0.0            16991          5     3398.2     1240.0        80     12261       5105.0  fread                 \n",
      "      0.0            16180          2     8090.0     8090.0      4780     11400       4681.0  socket                \n",
      "      0.0            11511          1    11511.0    11511.0     11511     11511          0.0  connect               \n",
      "      0.0             6940          1     6940.0     6940.0      6940      6940          0.0  pipe2                 \n",
      "      0.0             5670         64       88.6       50.0        40       440         63.7  pthread_mutex_trylock \n",
      "      0.0             2080          1     2080.0     2080.0      2080      2080          0.0  bind                  \n",
      "      0.0             1270          1     1270.0     1270.0      1270      1270          0.0  listen                \n",
      "      0.0              290          1      290.0      290.0       290       290          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)        Name       \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  -----------------\n",
      "    100.0        108499938          1  108499938.0  108499938.0  108499938  108499938          0.0  cudaMallocManaged\n",
      "      0.0            40292          1      40292.0      40292.0      40292      40292          0.0  cudaFree         \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "SKIPPED: /dli/task/report6.sqlite does not contain CUDA kernel data.\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "SKIPPED: /dli/task/report6.sqlite does not contain GPU memory data.\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "SKIPPED: /dli/task/report6.sqlite does not contain GPU memory data.\n",
      "Generated:\n",
      "    /dli/task/report6.nsys-rep\n",
      "    /dli/task/report6.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-46cc.qdstrm'\n",
      "[1/8] [========================100%] report7.nsys-rep\n",
      "[2/8] [========================100%] report7.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report7.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.3       6154437319        318  19353576.5  10071614.0      2810  100165225   27629497.0  poll                  \n",
      "      8.7        595579388        283   2104520.8   2065094.0       280   20501012    1439280.5  sem_timedwait         \n",
      "      0.6         42661073        499     85493.1     12340.0       380    9780772     582618.2  ioctl                 \n",
      "      0.3         19420907         24    809204.5      5795.0       930    7326331    2179422.7  mmap                  \n",
      "      0.0           970031         27     35927.1      4450.0      2860     629816     119366.7  mmap64                \n",
      "      0.0           528691         44     12015.7     10890.5      3140      34111       5265.8  open64                \n",
      "      0.0           193217          4     48304.3     46366.5     35121      65363      15471.3  pthread_create        \n",
      "      0.0           190928         29      6583.7      4170.0      1420      36481       7857.3  fopen                 \n",
      "      0.0           119953         11     10904.8     14380.0      1100      20711       7357.0  write                 \n",
      "      0.0            66303         11      6027.5      3650.0      1430      19631       5720.7  munmap                \n",
      "      0.0            51362         26      1975.5        70.0        60      49482       9689.5  fgets                 \n",
      "      0.0            39591          6      6598.5      7335.5      2540       9620       2675.5  open                  \n",
      "      0.0            37741         52       725.8       520.0       160       6430        883.7  fcntl                 \n",
      "      0.0            27950         22      1270.5      1055.0       600       4840        895.1  fclose                \n",
      "      0.0            22512         14      1608.0      1365.0       540       3941       1063.2  read                  \n",
      "      0.0            18870          2      9435.0      9435.0      6880      11990       3613.3  socket                \n",
      "      0.0            12491          1     12491.0     12491.0     12491      12491          0.0  connect               \n",
      "      0.0             9101          5      1820.2      1110.0        80       5621       2284.2  fread                 \n",
      "      0.0             7821          1      7821.0      7821.0      7821       7821          0.0  pipe2                 \n",
      "      0.0             6210         64        97.0        80.0        40        420         66.6  pthread_mutex_trylock \n",
      "      0.0             2900          1      2900.0      2900.0      2900       2900          0.0  bind                  \n",
      "      0.0             1240          1      1240.0      1240.0      1240       1240          0.0  listen                \n",
      "      0.0              300          1       300.0       300.0       300        300          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     94.6       2471227945          1  2471227945.0  2471227945.0  2471227945  2471227945          0.0  cudaDeviceSynchronize\n",
      "      4.7        122137718          3    40712572.7       58852.0       14581   122064285   70452653.0  cudaMallocManaged    \n",
      "      0.7         19457249          3     6485749.7     6209605.0     5863301     7384343     797235.3  cudaFree             \n",
      "      0.0            54782          1       54782.0       54782.0       54782       54782          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2471218344          1  2471218344.0  2471218344.0  2471218344  2471218344          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.6         34160024   2304   14826.4    6112.5      1791     88578      22519.0  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.4         11051148    768   14389.5    3759.5      1215     80514      22782.1  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report7.nsys-rep\n",
      "    /dli/task/report7.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-2ee5.qdstrm'\n",
      "[1/8] [========================100%] report8.nsys-rep\n",
      "[2/8] [========================100%] report8.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report8.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6045123622        316  19130138.0  10072094.5      3210  100159759   27363668.5  poll                  \n",
      "      8.8        590483315        282   2093912.5   2065062.0       230   20440611    1263117.9  sem_timedwait         \n",
      "      0.5         33272161        499     66677.7     12900.0       420    9340274     435991.9  ioctl                 \n",
      "      0.3         19237692         24    801570.5      5425.0      1140    7188772    2156071.0  mmap                  \n",
      "      0.0          1124391         27     41644.1      4481.0      3400     751772     142662.2  mmap64                \n",
      "      0.0           529441         44     12032.8     11255.0      4300      34541       5655.3  open64                \n",
      "      0.0           195938          4     48984.5     49042.0     37771      60083      12359.5  pthread_create        \n",
      "      0.0           176946         29      6101.6      3880.0      1590      28672       6484.9  fopen                 \n",
      "      0.0           147755         11     13432.3     14391.0      1080      24931       6332.4  write                 \n",
      "      0.0            83883         11      7625.7      3560.0      1130      40331      11378.4  munmap                \n",
      "      0.0            59993         26      2307.4        90.0        70      57783      11314.9  fgets                 \n",
      "      0.0            40714          6      6785.7      6346.0      3541      10050       2665.2  open                  \n",
      "      0.0            37564         52       722.4       575.0       160       6900        922.2  fcntl                 \n",
      "      0.0            34122         22      1551.0      1310.0       760       4731        872.0  fclose                \n",
      "      0.0            24482         14      1748.7      1380.0       620       4550       1211.0  read                  \n",
      "      0.0            17040          2      8520.0      8520.0      3980      13060       6420.5  socket                \n",
      "      0.0            12650          1     12650.0     12650.0     12650      12650          0.0  connect               \n",
      "      0.0             9860          5      1972.0      1310.0        90       5310       2188.8  fread                 \n",
      "      0.0             7050          1      7050.0      7050.0      7050       7050          0.0  pipe2                 \n",
      "      0.0             5791         64        90.5        50.5        40        340         54.2  pthread_mutex_trylock \n",
      "      0.0             2860          1      2860.0      2860.0      2860       2860          0.0  bind                  \n",
      "      0.0             1260          1      1260.0      1260.0      1260       1260          0.0  listen                \n",
      "      0.0              310          1       310.0       310.0       310        310          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     95.0       2471800872          1  2471800872.0  2471800872.0  2471800872  2471800872          0.0  cudaDeviceSynchronize\n",
      "      4.3        111484736          3    37161578.7       25511.0       13560   111445665   64331906.1  cudaMallocManaged    \n",
      "      0.7         19245711          3     6415237.0     6146559.0     5880358     7218794     708515.1  cudaFree             \n",
      "      0.0            51863          1       51863.0       51863.0       51863       51863          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2471791488          1  2471791488.0  2471791488.0  2471791488  2471791488          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34162742   2304   14827.6    4287.5      1854     88962      22515.0  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11062505    768   14404.3    3760.0      1279     80802      22788.1  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report8.nsys-rep\n",
      "    /dli/task/report8.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-b1aa.qdstrm'\n",
      "[1/8] [========================100%] report9.nsys-rep\n",
      "[2/8] [========================100%] report9.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report9.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6154884536        318  19354982.8  10070489.0      2051  100169847   27644901.3  poll                  \n",
      "      8.7        595397313        283   2103877.4   2065409.0       180   20478483    1430849.9  sem_timedwait         \n",
      "      0.5         37363590        499     74876.9     10621.0       380    8255876     518039.6  ioctl                 \n",
      "      0.3         19932179         24    830507.5      5725.5      1040    7347977    2234347.4  mmap                  \n",
      "      0.0           940973         27     34850.9      4020.0      3000     583146     110486.3  mmap64                \n",
      "      0.0           519922         44     11816.4     10840.5      4700      29551       4696.7  open64                \n",
      "      0.0           176246          4     44061.5     44646.5     33661      53292       9950.4  pthread_create        \n",
      "      0.0           170099         29      5865.5      3891.0      1591      38722       7459.0  fopen                 \n",
      "      0.0           140944         11     12813.1     13280.0       800      33891       9350.4  write                 \n",
      "      0.0            84623         11      7693.0      3830.0      1360      33561       9958.0  munmap                \n",
      "      0.0            51472         26      1979.7        70.0        60      49652       9723.3  fgets                 \n",
      "      0.0            35691          6      5948.5      6700.5      2600       8160       2282.8  open                  \n",
      "      0.0            33292         52       640.2       465.0       170       5530        754.5  fcntl                 \n",
      "      0.0            24630         22      1119.5       995.0       530       2930        563.3  fclose                \n",
      "      0.0            21422         14      1530.1      1335.0       471       3440        859.8  read                  \n",
      "      0.0            15931          2      7965.5      7965.5      3720      12211       6004.0  socket                \n",
      "      0.0            11351          1     11351.0     11351.0     11351      11351          0.0  connect               \n",
      "      0.0             7611          1      7611.0      7611.0      7611       7611          0.0  pipe2                 \n",
      "      0.0             6991          5      1398.2      1120.0        80       3461       1428.2  fread                 \n",
      "      0.0             6780         64       105.9       115.0        40        470         80.5  pthread_mutex_trylock \n",
      "      0.0             2150          1      2150.0      2150.0      2150       2150          0.0  bind                  \n",
      "      0.0             1480          1      1480.0      1480.0      1480       1480          0.0  listen                \n",
      "      0.0              290          1       290.0       290.0       290        290          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     94.8       2471760429          1  2471760429.0  2471760429.0  2471760429  2471760429          0.0  cudaDeviceSynchronize\n",
      "      4.4        115926307          3    38642102.3       55822.0       14911   115855574   66868831.1  cudaMallocManaged    \n",
      "      0.8         19997122          3     6665707.3     6510290.0     6074882     7411950     681948.4  cudaFree             \n",
      "      0.0            46122          1       46122.0       46122.0       46122       46122          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2471750879          1  2471750879.0  2471750879.0  2471750879  2471750879          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34143994   2304   14819.4    4367.5      1983     80514      22495.0  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11063565    768   14405.7    3711.5      1279     80961      22792.8  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report9.nsys-rep\n",
      "    /dli/task/report9.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-2975.qdstrm'\n",
      "[1/8] [========================100%] report10.nsys-rep\n",
      "[2/8] [========================100%] report10.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report10.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.3       6153995411        318  19352186.8  10069918.0      2760  100152978   27636083.2  poll                  \n",
      "      8.8        597306693        283   2110624.4   2064941.0       120   20566435    1502932.6  sem_timedwait         \n",
      "      0.6         39398849        499     78955.6     13560.0       370    9426475     521935.9  ioctl                 \n",
      "      0.3         19160014         24    798333.9      5375.0      1190    7213967    2152662.6  mmap                  \n",
      "      0.0          1097769         27     40658.1      4760.0      3500     740062     140433.9  mmap64                \n",
      "      0.0           545678         44     12401.8     11285.5      4840      38612       5694.5  open64                \n",
      "      0.0           200040          4     50010.0     50062.5     38442      61473      13059.7  pthread_create        \n",
      "      0.0           163547         29      5639.6      3910.0      1481      23251       5036.5  fopen                 \n",
      "      0.0           149316         11     13574.2     12880.0      1150      22991       5868.4  write                 \n",
      "      0.0           111144         12      9262.0      4190.0      1500      37921      10694.5  munmap                \n",
      "      0.0            75924         26      2920.2        90.0        70      73554      14406.5  fgets                 \n",
      "      0.0            43101          6      7183.5      6880.5      3980      10890       2669.6  open                  \n",
      "      0.0            36912         52       709.8       530.0       160       6310        854.5  fcntl                 \n",
      "      0.0            31450         22      1429.5      1260.0       720       3640        671.8  fclose                \n",
      "      0.0            23441         14      1674.4      1475.0       730       3791        962.0  read                  \n",
      "      0.0            17300          2      8650.0      8650.0      3850      13450       6788.2  socket                \n",
      "      0.0            13051          1     13051.0     13051.0     13051      13051          0.0  connect               \n",
      "      0.0             8420          5      1684.0      1520.0        90       3520       1516.0  fread                 \n",
      "      0.0             7111          1      7111.0      7111.0      7111       7111          0.0  pipe2                 \n",
      "      0.0             6280         64        98.1        50.0        40        430         77.4  pthread_mutex_trylock \n",
      "      0.0             2330          1      2330.0      2330.0      2330       2330          0.0  bind                  \n",
      "      0.0             1600          1      1600.0      1600.0      1600       1600          0.0  listen                \n",
      "      0.0              331          1       331.0       331.0       331        331          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     94.7       2469596502          1  2469596502.0  2469596502.0  2469596502  2469596502          0.0  cudaDeviceSynchronize\n",
      "      4.5        117831156          3    39277052.0       41942.0       13891   117775323   67981498.3  cudaMallocManaged    \n",
      "      0.7         19243297          3     6414432.3     6138880.0     5846817     7257600     744663.7  cudaFree             \n",
      "      0.0           109074          1      109074.0      109074.0      109074      109074          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2469647321          1  2469647321.0  2469647321.0  2469647321  2469647321          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34142395   2304   14818.7    4383.5      1983     80355      22493.6  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11063685    768   14405.8    3775.5      1279     80738      22787.3  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report10.nsys-rep\n",
      "    /dli/task/report10.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](09-saxpy/01-saxpy.cu). It currently works and you can compile, run, and then profile it with `nsys profile` below.\n",
    "\n",
    "Record the runtime of the `saxpy` kernel without making any modifications and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200,000 ns*. Check out [the solution](09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Generating '/tmp/nsys-report-6c59.qdstrm'\n",
      "[1/8] [========================100%] report11.nsys-rep\n",
      "[2/8] [========================100%] report11.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report11.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ---------  ----------  --------  --------  -----------  ----------------------\n",
      "     69.4        190753371         21  9083493.9  10066988.0      2560  47068833   10430287.6  poll                  \n",
      "     16.2         44649764         19  2349987.6    229301.0       160  20459149    5008192.2  sem_timedwait         \n",
      "     12.8         35066636        497    70556.6     10440.0       380   8209655     472789.2  ioctl                 \n",
      "      0.8          2208789         23    96034.3      7050.0      1100    701282     235238.3  mmap                  \n",
      "      0.3           889118         27    32930.3      3890.0      2990    556534     105378.6  mmap64                \n",
      "      0.2           478755         44    10880.8      9890.5      2930     32661       5270.3  open64                \n",
      "      0.1           182088          4    45522.0     43036.5     31352     64663      15958.8  pthread_create        \n",
      "      0.1           170079         29     5864.8      3420.0      1390     37222       7440.3  fopen                 \n",
      "      0.1           148425         11    13493.2     13590.0       990     21101       5329.6  write                 \n",
      "      0.0            50792         26     1953.5        70.0        60     49022       9600.1  fgets                 \n",
      "      0.0            40872          6     6812.0      8200.5      2451      9410       2931.3  open                  \n",
      "      0.0            36572         14     2612.3      1275.0       710     14301       3544.5  read                  \n",
      "      0.0            32520         52      625.4       445.0       190      6260        845.0  fcntl                 \n",
      "      0.0            27902          9     3100.2      2960.0      1680      5580       1241.0  munmap                \n",
      "      0.0            26791         22     1217.8       990.0       530      3130        658.6  fclose                \n",
      "      0.0            16411          2     8205.5      8205.5      4161     12250       5719.8  socket                \n",
      "      0.0            15962          5     3192.4       260.0        80     11761       5020.3  fread                 \n",
      "      0.0            10851          1    10851.0     10851.0     10851     10851          0.0  connect               \n",
      "      0.0             8311          1     8311.0      8311.0      8311      8311          0.0  pipe2                 \n",
      "      0.0             5790         64       90.5        50.0        40       450         66.0  pthread_mutex_trylock \n",
      "      0.0             2360          1     2360.0      2360.0      2360      2360          0.0  bind                  \n",
      "      0.0             1370          1     1370.0      1370.0      1370      1370          0.0  listen                \n",
      "      0.0              280          1      280.0       280.0       280       280          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)        Name       \n",
      " --------  ---------------  ---------  ----------  --------  --------  ---------  -----------  -----------------\n",
      "     88.6        111974200          3  37324733.3   21761.0     17841  111934598   64614038.2  cudaMallocManaged\n",
      "     11.3         14294206          3   4764735.3  723632.0    697081   12873493    7022402.7  cudaFree         \n",
      "      0.1           109105          1    109105.0  109105.0    109105     109105          0.0  cudaLaunchKernel \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  --------------------------\n",
      "    100.0         12207914          1  12207914.0  12207914.0  12207914  12207914          0.0  saxpy(int *, int *, int *)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0          8137116   2652    3068.3    2175.0      1823     67970       3150.4  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     50.332   2652     0.019     0.004     0.004     0.885        0.042  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report11.nsys-rep\n",
      "    /dli/task/report11.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
